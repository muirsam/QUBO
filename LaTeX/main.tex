\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsmath, amsfonts, amssymb}
\usepackage[dvipsnames]{xcolor}
\usepackage{bbold}
\usepackage{float}

% ========= Bibliography =========
% These lines load the `biblatex' package
% and read in the list of references from
% References.bib - take a look.
%
% To generate References.bib, I recommend https://www.mybib.com/
% rather than trying to write the .bib file yourself.
%
\usepackage{csquotes, biblatex}
\addbibresource{References.bib}
% ================================

\usepackage[pdffitwindow=false,
             plainpages=false,
             pdfpagelabels=true,
             pdfpagemode=UseOutlines,
             pdfpagelayout=SinglePage,
             bookmarks=false,
             colorlinks=true,
             hyperfootnotes=false,
             linkcolor=blue,
             urlcolor=blue!30!black,
             citecolor=green!50!black]{hyperref}

\newtheorem{prop}{Proposition} % This defines a theorem-like environment. See https://www.overleaf.com/learn/latex/theorems_and_proofs
\newtheorem{lem}[prop]{Lemma}
\newtheorem{thm}[prop]{Theorem}
\newtheorem{cor}[prop]{Corollary}
\newtheorem{defn}[prop]{Definition}

\title{Quadratic Unconstrained Binary Optimization}
\author{Sam Muir }
\date{June 2024}

\begin{document}

\maketitle

\section{Introduction}

This project will begin by covering how QUBO models are constructed. Then we explore how QUBO models can be applied to the graph matching problem and the related quadratic assignment problem. Finally, we will discuss the application of 'quantum annealers' to solve QUBO problems. 

This project will follow ideas from 'Continuous optimization methods for the graph isomorphism problem' by Stefan Klus and Patrick Gelß.

\section{What are QUBO problems?}
\begin{defn}
A QUBO problem is an optimisation problem of form, 
\begin{align*}
    \min \: f(\mathbf{x}) = \mathbf{x}^T Q \mathbf{x}
\end{align*}
where \(\mathbf{x}\) is a vector of \(n\) binary variables and \(Q\) is a \(n \times n\) matrix.
\end{defn}

\noindent These are important because many famous optimisation problems can be expressed as QUBO problems. 

\subsection{Linear Assignment problem}

The linear assignment problem can be stated as:

Suppose we have \(n\) machines and \(n\) tasks which must be completed. We can assign any machine to do any task, but each task-machine pair has an associated cost. We want to allocate one machine to each task such that the sum of the costs is minimised. \\

\noindent If we let \(A\) be the matrix of machine-task costs then the problem can expressed as: 
\begin{align*}
    \min \: g(P) = \text{tr}(PA)
\end{align*}
where \(P\) is a permutation matrix which represents a task allocation. \\

\noindent We model this problem as a QUBO as follows: \\ 

\noindent First, let \(A = \begin{pmatrix}
    a_{11} & \hdots & a_{1n} \\
    \vdots & \ddots & \vdots \\
    a_{n1} & \hdots & a_{nn}
\end{pmatrix}\) and \(P = \begin{pmatrix}
    x_{1} & x_2 & \hdots & x_{n} \\
    x_{n + 1} & x_{n + 2} & \hdots & x_{2n} \\
    \vdots & \vdots & \ddots & \vdots \\
    x_{n(n-1)+1} & x_{n(n-1)+2} & \hdots & x_{n^2}
\end{pmatrix}\).
Then the function to be minimized is,
\begin{align*}
    \text{tr} (PA) &= \begin{pmatrix}
        x_1 \\
        \vdots \\
        x_n
    \end{pmatrix} \cdot \begin{pmatrix}
        a_{11}\\
        \vdots \\
        a_{n1}
    \end{pmatrix} + \begin{pmatrix}
        x_{n+1} \\
        \vdots \\
        x_{2n}
    \end{pmatrix} \cdot \begin{pmatrix}
        a_{12}\\
        \vdots \\
        a_{n2}
    \end{pmatrix} + \hdots + \begin{pmatrix}
        x_{n(n-1) + 1} \\
        \vdots \\
        x_{n^2}
    \end{pmatrix} \cdot \begin{pmatrix}
        a_{1n}\\
        \vdots \\
        a_{nn}
    \end{pmatrix} \\
    &= \sum_{i=1}^{n} \begin{pmatrix}
        x_{n(i-1) + 1} \\
        \vdots \\
        x_{ni}
    \end{pmatrix} \cdot \begin{pmatrix}
        a_{1i} \\
        \vdots \\
        a_{ni}
    \end{pmatrix} \\
    &= \sum_{i=1}^{n} \sum_{j=1}^{n} a_{ji}x_{n(i-1)+j}
\end{align*}
But since \(x_i\) is binary, \(x_i = x_i^2\), and so 
\begin{align*}
    \text{tr}(PA) &= \sum_{i=1}^{n} \sum_{j=1}^{n} a_{ji}x_{n(i-1)+j} \\
    &= \mathbf{x}^T \text{diag}(\text{vec}(A))\mathbf{x}.
\end{align*}\\
\noindent So we can now write the problem as 
\begin{align} \label{Ex:LinAss 1}
    \min g(\mathbf{x}) = \mathbf{x}^T \text{diag}(\text{vec}(A)) \mathbf{x}
\end{align}
where \(\mathbf{x} = \text{vec}(P^T)\) and \(P\) is a permutation matrix.\\

\noindent Now we need a penalty function which forces \(P\) to be a permutation matrix. To accomplish this we use the equation \(C\mathbf{x} = d\) given in \cite[p.~8]{klus2023continuous} where 
\begin{align*}
    C = \begin{pmatrix}
        \mathbb{1}_n^T & & \\
         & \ddots & \\ 
         & & \mathbb{1}_n^T \\
         e_1^T & \hdots & e_1^T \\
         \vdots & \ddots & \vdots \\
         e_n^T & \hdots & e_n^T
    \end{pmatrix} \in \mathbb{R}^{2n \times n^2}
\end{align*}
and \(d = \mathbb{1}_{2n}\). \\
This equation is 0 if and only if \(P\) is doubly stochastic. But if \(P\) is doubly stochastic it is also a permutation matrix since each entry is binary. So this constraint forces \(P\) to be a permutation matrix.

\newpage
\noindent We can transform the constraint \(C\mathbf{x} = d\) into the penalty \((C\mathbf{x} - d) \cdot (C\mathbf{x} - d)\). Adding this to (\ref{Ex:LinAss 1}) will favour solutions which are  permutation matrices.
Expanding this penalty we get,
\begin{align*}
    (C\mathbf{x} - d) \cdot (C\mathbf{x} - d) &= C\mathbf{x} \cdot C\mathbf{x} - 2d\cdot C\mathbf{x} + d \cdot d \\
    &= \mathbf{x}^T C^T C \mathbf{x} -2\mathbf{x}^T C^T d + d\cdot d
\end{align*}

\noindent Then since \(\mathbf{x}\) is binary \(x_i = x_i^2\) the linear term \(-2\mathbf{x}^T C^T d\) can be expressed as the quadratic term \(-2\mathbf{x}^T\text{diag}(C^Td)\mathbf{x}\). So the penalty term becomes
\begin{align*}
    &\mathbf{x}^T C^T C \mathbf{x} -2\mathbf{x}^T\text{diag}(C^Td)\mathbf{x} + d\cdot d \\
    = \: &\mathbf{x}^T(C^T C -2\text{diag}(C^Td))\mathbf{x} + d\cdot d.
\end{align*}
And so we can rewrite problem (\ref{Ex:LinAss 1}) as 
\begin{align*}
    \min g(\mathbf{x}) = \mathbf{x}^T (\text{diag}(\text{vec}(A)) + \lambda C^T C - 2\lambda\text{diag}(C^T d))\mathbf{x} + \lambda d\cdot d
\end{align*}
where \(\lambda\) is the penalty weight. Then to get it in QUBO form we drop the consant to get
\begin{align*}
    \min g(\mathbf{x}) = \mathbf{x}^T Q \mathbf{x}
\end{align*}
where \(Q = \text{diag}(\text{vec}(A) - 2\lambda C^Td) + \lambda C^T C\).
\subsubsection{Example: Linear Assignment Problem}

Consider the cost matrix, \begin{align*}
    A = \begin{pmatrix}
        7 & 9 & 1\\
        4 & 2 & 6\\
        7 & 8 & 7
    \end{pmatrix}
\end{align*}
where each column corresponds to a task and each row corresponds to the machine costs. Using \(A\) and setting \(\lambda = 10\) we obtain the matrix
\begin{align*}
    Q = \begin{pmatrix}
        -13 & 10 & 10 & 10 & 0 & 0 & 10 & 0 & 0 \\
        10 & -16 & 10 & 0 & 10 & 0 & 0 & 10 & 0 \\
        10 & 10 & -13 & 0 & 0 & 10 & 0 & 0 & 10 \\
        10 & 0 & 0 & -11 & 10 & 10 & 10 & 0 & 0 \\
        0 & 10 & 0 & 10 & -18 & 10 & 0 & 10 & 0 \\
        0 & 0 & 10 & 10 & 10 & -12 & 0 & 0 & 10 \\
        10 & 0 & 0 & 10 & 0 & 0 & -19 & 10 & 10 \\
        0 & 10 & 0 & 0 & 10 & 0 & 10 & -14 & 10 \\
        0 & 0 & 10 & 0 & 0 & 10 & 10 & 10 & -13
    \end{pmatrix}
\end{align*}
Solving this gives \(X = \begin{pmatrix}
    0 & 0 & 1 & 0 & 1 & 0 & 1 & 0 & 0
\end{pmatrix}^T\)
and so we have \begin{align*}
    P = \begin{pmatrix}
        0 & 0 & 1 \\
        0 & 1 & 0 \\
        1 & 0 & 0
    \end{pmatrix}, \quad PA = \begin{pmatrix}
        7 & 8 & 7 \\
        4 & 2 & 6 \\
        7 & 9 & 1
    \end{pmatrix}
\end{align*}
which tells us that the minimum cost is \(\text{tr}(PA) = 10\).
\subsection{Sudoku}

In the game of Sudoku we are given a partially filled \(9 \times 9\) grid (81 squares) and the aim is to populate the remainder of the grid such that every column, row and box contains each of the numbers from 1 to 9. These boxes split the \(9 \times 9\) grid into 9 non-overlapping \(3 \times 3\) grids.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{sudoku.png}
    \caption{Medium Sudoku puzzle from New York Times on June 24, 2024}
    \label{fig:Sudoku 1}
\end{figure}

\noindent We can formulate the Sudoku problem as follows. Following \cite[p.~1]{mücke2024sudoku} the vector \(\mathbf{x}\) will have 9 binary entries for each Sudoku square \(x_{i,j,k} = x_{81i + 9j + k}\). So each variable \(x_{i,j,k}\) is defined as
\begin{align*}
    x_{i,j,k} = \begin{cases}
        1 & \text{if row \(i\) column \(j\) has value \(k\)} \\
        0 & \text{otherwise.}
    \end{cases}
\end{align*}
The constraints of this problem are:
\begin{center}
\begin{tabular}{ |c|c| } 
 \hline
   & Constraint\\ 
 \hline
 1 & Each column contains 1 to 9\\ 
 2 & Each row contains 1 to 9\\
 3 & Each box contains 1 to 9\\
 4 & Every square must have a value\\
 \hline
\end{tabular}
\end{center}

\noindent Using \cite[p.~326]{ILPsudoku} the constraints above become
\begin{equation*}
\begin{aligned} 
\text{(1)} \quad \sum_{i=1}^9 x_{i,j,k} &= 1\quad \forall j,k \in \{1 \dots 9\} \\
\end{aligned}
\end{equation*}


\begin{equation*}
\begin{aligned} 
&\text{(2)} \quad \sum_{j=1}^9 x_{i,j,k} &&= 1\quad \forall i,k \in \{1 \dots 9\} \\
&\text{(3)} \quad \sum_{j=3q-2}^{3q} \sum_{i=3p-2}^{3p} x_{i,j,k} &&= 1\quad \forall k \in \{1 \dots 9\}, \forall p,q \in \{1 \dots 3\}\\
&\text{(4)} \quad \sum_{k=1}^9 x_{i,j,k} &&= 1\quad \forall i,j \in \{1 \dots 9\} \\
\end{aligned}
\end{equation*}
where \(F\) is the set containing all triples corresponding to the indices of the filled squares. \\

\noindent We can transform these constraints into penalty functions by subtracting 1 and squaring them. Then to obtain a solution to these constraints we minimise the sum of these penalties. This also means that we don't need any penalty weights.\\

\noindent Consider the 1st constraint for some fixed \(j\) and \(k\). Expanding the corresponding penalty gives us
\begin{align*}
    \bigg(\sum_{i=1}^9 x_{i,j,k} - 1\bigg)^2 &= \sum_{i=1}^9 x_{i,j,k}\sum_{p=1}^9 x_{p,j,k} -2\sum_{i=1}^9 x_{i,j,k} + 1  \\
    &= \sum_{i=1}^9\sum_{p=1}^9 x_{i,j,k}x_{p,j,k} -2\sum_{i=1}^9 x_{i,j,k}^2 + 1 \\
    &= \begin{pmatrix}
        x_{1,j,k} & \dots & x_{9,j,k}
    \end{pmatrix} (J_9 - 2I) \begin{pmatrix}
        x_{1,j,k} \\
        \vdots \\
        x_{9,j,k}
    \end{pmatrix} + 1
\end{align*}
where \(J_n\) is the \(n \times n\) matrix of ones. \\

\noindent This we can replace \(\begin{pmatrix}
        x_{1,j,k} & \dots & x_{9,j,k}
    \end{pmatrix}^T\) with \(\mathbf{x}\) if we
    embed \(J_9 - 2I\) in a larger matrix with zeros everywhere but the rows and columns representing \(x_{1,j,k} \dots x_{9,j,k}\).\\

\noindent If we do this for every constraint and drop the constant we can add all 324 matrices to get a \(729 \times 729\) matrix \(U\)\footnote{This matrix is provided on the GitHub page under the name 'sudokuQ.mat' }.\\

\noindent Then we can take the squares which have already been filled and turn them into constraints. So the constraint \(x_{i,j,k} = 1\) becomes 
\begin{align*}
    (x_{i,j,k} - 1)^2 &= x_{i,j,k}^2 - 2x_{i,j,k} + 1 \\
    &= -x_{i,j,k}^2 + 1
\end{align*}
which can be interpreted as subtracting 1 from \((i,j,k)\)th element on the diagonal of \(U\). Doing this for every square which has been filled creates the matrix \(Q\).\\

\noindent Finally, solving the QUBO problem
\begin{align*}
    \min \mathbf{x}^T Q \mathbf{x}
\end{align*}
will give us the vector \(\mathbf{x}\) which is the solution of the original Sudoku problem.
 

\section{Graph Isomorphism and Matching problems}
\subsection{The Graph Isomorphism problem}
Two graphs \(G_A\) and \(G_B\) are isomorphic iff there exists a bijection between the vertices of the graphs such that every edge connecting vertices in \(G_A\) corresponds to a edge connecting vertices in \(G_B\). \\
Essentially, isomorphism means each graph can be rearranged and relabelled to look identical to the other. \\

\noindent Look at these two graphs, 


\noindent First we need to establish some results from \autocite{klus2023continuous}.
\begin{defn}\label{def:1}
    \cite[p.~6]{klus2023continuous} Suppose we have graphs \(G_A\) and \(G_B\) with adjacency matrices \(A\) and \(B\) respectively. The doubly stochastic relaxation of the graph isomorphism problem can be formulated as
    \begin{equation*}
        c_D = \min_{X \in D(n)} ||XA - BX||^2_F
    \end{equation*}
    where \(D(n)\) is the set of doubly stochastic matrices and \(||\cdot||_F\) denotes the Frobenius norm.
\end{defn}

\begin{lem}
    \cite[p.~13]{klus2023continuous} If we penalize non-binary matrices, this problem can be written as \begin{align*}
    \min_{\substack{\mathbf{x}\geq 0 \\ C\mathbf{x} = d \\ H\mathbf{x} = 0}} -\mathbf{x}^T \mathbf{x}
    \end{align*}
    with \(\mathbf{x} = \begin{bmatrix}
        X_1 \\
        \vdots \\
        X_n
    \end{bmatrix}\) where \(X_1, \dots X_n\) are the columns of \(X\) and \(C\), \(d\), and H are defined as shown on \cite[p.~8]{klus2023continuous}.
\end{lem}

\noindent To obtain a QUBO formulation of this problem we can transform the constraints into penalties as shown below.
\begin{center}
\begin{tabular}{ |c|c|c| } 
 \hline
   & Constraint  & Penalty \\ 
 \hline
 1 & \(C\mathbf{x} = d\) & \((C\mathbf{x} - d)\cdot(C\mathbf{x} - d)\) \\ 
 2 & \(H\mathbf{x} = 0\) & \(H \mathbf{x} \cdot H \mathbf{x} \) \\ 
 \hline
\end{tabular}
\end{center}
These penalties are 0 for functions which satisfy the constraints and positively-valued otherwise. Therefore by scaling each penalty by the weights \(\lambda_1\) and \(\lambda_2\) then adding them to the objective function we can ensure that the optimal solution to the QUBO problem corresponds to a isomorphism (if it exists).\\

\noindent Penalty 1 can be simplified to,
\begin{align*}
    (C\mathbf{x} - d)\cdot (C\mathbf{x} - d) &= C\mathbf{x} \cdot C\mathbf{x} - 2d\cdot C\mathbf{x} - d\cdot d \\
    &= \mathbf{x}^T C^T C \mathbf{x} - 2d\cdot Cx - d\cdot d.
\end{align*}
But \(\mathbf{x}\) is a binary vector, so \(\mathbf{x}_i = \mathbf{x}_i^2\) for all \(i\) and therefore the linear term \(-2d\cdot C\mathbf{x}\) can be expressed as a quadratic term. So we can write \(-2d\cdot C \mathbf{x} = -2\mathbf{x}^T\text{diag}(C^Td)\mathbf{x}\) and thus penalty 1 becomes
\begin{align*}
    (C\mathbf{x} - d)\cdot (C\mathbf{x} - d) = \mathbf{x}^T( C^T C -2\text{diag}(C^T d))\mathbf{x} - d\cdot d
\end{align*}
where diag\((v)\) is the function which maps the \(i\)th entry of \(v\) to the \(i\)th diagonal entry of a diagonal matrix.\\

\noindent Penalty 2 can be written as,
\begin{align*}
    H\mathbf{x} \cdot H\mathbf{x} = \mathbf{x}^T H^T H \mathbf{x}.
\end{align*}

\noindent So the QUBO formulation of this problem is 
\begin{align*}
    &-\mathbf{x}^T \mathbf{x} + \lambda_1(C\mathbf{x} - d)\cdot (C\mathbf{x} - d)  +\lambda_2 H\mathbf{x} \cdot H\mathbf{x}\\
    = &-\mathbf{x}^T \mathbf{x} + \lambda_1(\mathbf{x}^T( C^T C -2\text{diag}(C^T d))\mathbf{x} - d\cdot d) + \lambda_2 \mathbf{x}^T H^T H \mathbf{x}\\
    = &\hspace{5pt} \mathbf{x}^T (-I + \lambda_1( C^T C -2\text{diag}(C^T d)) + 
    \lambda_2 H^T H) \mathbf{x} - \lambda_1 d \cdot d.
\end{align*}\\
We can drop the constant \(- \lambda_1 d \cdot d\) at the end and then write the problem as
\begin{align*}
    \min \: f(\mathbf{x}) = \mathbf{x}^T Q \mathbf{x}
\end{align*}
where \(Q = -I + \lambda_1( C^T C -2\text{diag}(C^T d)) + \lambda_2 H^T H\).\\

\noindent Note that depending on the penalty weights used the optimal solution may not correspond to an isomorphism. So each optimal solution should be tested against the constraints.

\subsection{Example: Graph Isomorphism problem}






\nocite{*}
\printbibliography % This command prints the cited references.

\end{document}
