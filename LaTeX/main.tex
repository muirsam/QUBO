\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsmath,amsfonts,amssymb}
\usepackage[dvipsnames]{xcolor}
\usepackage{bbold}
\usepackage{float}

% ========= Bibliography =========
% These lines load the `biblatex' package
% and read in the list of references from
% References.bib - take a look.
%
% To generate References.bib, I recommend https://www.mybib.com/
% rather than trying to write the .bib file yourself.
%
\usepackage{csquotes,biblatex}
\addbibresource{References.bib}
% ================================

\usepackage[pdffitwindow=false,
             plainpages=false,
             pdfpagelabels=true,
             pdfpagemode=UseOutlines,
             pdfpagelayout=SinglePage,
             bookmarks=false,
             colorlinks=true,
             hyperfootnotes=false,
             linkcolor=blue,
             urlcolor=blue!30!black,
             citecolor=green!50!black]{hyperref}

\newtheorem{prop}{Proposition} % This defines a theorem-like environment. See https://www.overleaf.com/learn/latex/theorems_and_proofs
\newtheorem{lem}[prop]{Lemma}
\newtheorem{thm}[prop]{Theorem}
\newtheorem{cor}[prop]{Corollary}
\newtheorem{defn}[prop]{Definition}

\title{Quadratic Unconstrained Binary Optimization}
\author{Sam Muir }
\date{June 2024}

\begin{document}

\maketitle

\section{Introduction}

This project will begin by covering how QUBO models are constructed. Then we explore how QUBO models can be applied to the graph matching problem and the related quadratic assignment problem. Finally, we will discuss the application of 'quantum annealers' to solve QUBO problems. 

This project will follow ideas from 'Continuous optimization methods for the graph isomorphism problem' by Stefan Klus and Patrick Gelß.

\section{What are QUBO problems?}
\begin{defn}
A QUBO problem is an optimisation problem of form, 
\begin{align*}
    \min \: f(\mathbf{x}) = \mathbf{x}^T Q \mathbf{x}
\end{align*}
where \(\mathbf{x}\) is a vector of \(n\) binary variables and \(Q\) is a \(n \times n\) matrix.
\end{defn}

\noindent These are important because many famous optimisation problems can be expressed as QUBO problems. 

\subsection{Linear Assignment problem}

The linear assignment problem can be stated as:

Suppose we have \(n\) machines and \(n\) tasks which must be completed. We can assign any machine to do any task, but each task-machine pair has an associated cost. We want to allocate one machine to each task such that the sum of the costs is minimised. \\

\noindent If we let \(A\) be the matrix of machine-task costs then the problem can expressed as: 
\begin{align*}
    \min \: g(P) = \text{tr}(PA)
\end{align*}
where \(P\) is a permutation matrix which represents a task allocation. \\

\noindent We model this problem as a QUBO as follows: \\ 

\noindent First, let \(A = \begin{pmatrix}
    a_{11} & \hdots & a_{1n} \\
    \vdots & \ddots & \vdots \\
    a_{n1} & \hdots & a_{nn}
\end{pmatrix}\) and \(P = \begin{pmatrix}
    x_{1} & x_2 & \hdots & x_{n} \\
    x_{n + 1} & x_{n + 2} & \hdots & x_{2n} \\
    \vdots & \vdots & \ddots & \vdots \\
    x_{n(n-1)+1} & x_{n(n-1)+2} & \hdots & x_{n^2}
\end{pmatrix}\).
Then the function to be minimized is,
\begin{align*}
    \text{tr} (PA) &= \begin{pmatrix}
        x_1 \\
        \vdots \\
        x_n
    \end{pmatrix} \cdot \begin{pmatrix}
        a_{11}\\
        \vdots \\
        a_{n1}
    \end{pmatrix} + \begin{pmatrix}
        x_{n+1} \\
        \vdots \\
        x_{2n}
    \end{pmatrix} \cdot \begin{pmatrix}
        a_{12}\\
        \vdots \\
        a_{n2}
    \end{pmatrix} + \hdots + \begin{pmatrix}
        x_{n(n-1) + 1} \\
        \vdots \\
        x_{n^2}
    \end{pmatrix} \cdot \begin{pmatrix}
        a_{1n}\\
        \vdots \\
        a_{nn}
    \end{pmatrix} \\
    &= \sum_{i=1}^{n} \begin{pmatrix}
        x_{n(i-1) + 1} \\
        \vdots \\
        x_{ni}
    \end{pmatrix} \cdot \begin{pmatrix}
        a_{1i} \\
        \vdots \\
        a_{ni}
    \end{pmatrix} \\
    &= \sum_{i=1}^{n} \sum_{j=1}^{n} a_{ji}x_{n(i-1)+j}
\end{align*}
But since \(x_i\) is binary, \(x_i = x_i^2\), and so 
\begin{align*}
    \text{tr}(PA) &= \sum_{i=1}^{n} \sum_{j=1}^{n} a_{ji}x_{n(i-1)+j} \\
    &= \mathbf{x}^T \text{diag}(\text{vec}(A))\mathbf{x}.
\end{align*}\\
\noindent So we can now write the problem as 
\begin{align} \label{Ex:LinAss 1}
    \min g(\mathbf{x}) = \mathbf{x}^T \text{diag}(\text{vec}(A)) \mathbf{x}
\end{align}
where \(\mathbf{x} = \text{vec}(P^T)\) and \(P\) is a permutation matrix.\\

\noindent Now we need a penalty function which forces \(P\) to be a permutation matrix. To accomplish this we use the equation \(C\mathbf{x} = d\) given in \cite[p.~8]{klus2023continuous} where 
\begin{align*}
    C = \begin{pmatrix}
        \mathbb{1}_n^T & & \\
         & \ddots & \\ 
         & & \mathbb{1}_n^T \\
         e_1^T & \hdots & e_1^T \\
         \vdots & \ddots & \vdots \\
         e_n^T & \hdots & e_n^T
    \end{pmatrix} \in \mathbb{R}^{2n \times n^2}
\end{align*}
and \(d = \mathbb{1}_{2n}\). \\
This equation is 0 if and only if \(P\) is doubly stochastic. But if \(P\) is double stochastic it is also a permutation matrix since each entry is binary. Therefore this constraint forces each \(P\) to be a permutation matrix.

\newpage
\noindent We can transform the constraint \(C\mathbf{x} = d\) into the penalty \((C\mathbf{x} - d) \cdot (C\mathbf{x} - d)\). Adding this to (\ref{Ex:LinAss 1}) will favour solutions which are  permutation matrices.
Expanding this penalty we get,
\begin{align*}
    (C\mathbf{x} - d) \cdot (C\mathbf{x} - d) &= C\mathbf{x} \cdot C\mathbf{x} - 2d\cdot C\mathbf{x} + d \cdot d \\
    &= \mathbf{x}^T C^T C \mathbf{x} -2\mathbf{x}^T C^T d + d\cdot d
\end{align*}

\noindent Then since \(\mathbf{x}\) is binary \(x_i = x_i^2\) the linear term \(-2\mathbf{x}^T C^T d\) can be expressed as the quadratic term \(-2\mathbf{x}^T\text{diag}(C^Td)\mathbf{x}\). So the penalty term becomes
\begin{align*}
    &\mathbf{x}^T C^T C \mathbf{x} -2\mathbf{x}^T\text{diag}(C^Td)\mathbf{x} + d\cdot d \\
    = \: &\mathbf{x}^T(C^T C -2\text{diag}(C^Td))\mathbf{x} + d\cdot d.
\end{align*}
And so we can rewrite problem (\ref{Ex:LinAss 1}) as 
\begin{align*}
    \min g(\mathbf{x}) = \mathbf{x}^T (\text{diag}(\text{vec}(A)) + \lambda C^T C - 2\lambda\text{diag}(C^T d))\mathbf{x} + \lambda d\cdot d
\end{align*}
where \(\lambda\) is the penalty weight. Then to get it in QUBO form we drop the consant to get
\begin{align*}
    \min g(\mathbf{x}) = \mathbf{x}^T Q \mathbf{x}
\end{align*}
where \(Q = \text{diag}(\text{vec}(A) - 2\lambda C^Td) + \lambda C^T C\).
\subsubsection{Example: Linear Assignment Problem}

Consider the cost matrix, \begin{align*}
    A = \begin{pmatrix}
        7 & 9 & 1\\
        4 & 2 & 6\\
        7 & 8 & 7
    \end{pmatrix}
\end{align*}
where each column corresponds to a task and each row corresponds to the machine costs. Using \(A\) and setting \(\lambda = 10\) we obtain the matrix
\begin{align*}
    Q = \begin{pmatrix}
        -13 & 10 & 10 & 10 & 0 & 0 & 10 & 0 & 0 \\
        10 & -16 & 10 & 0 & 10 & 0 & 0 & 10 & 0 \\
        10 & 10 & -13 & 0 & 0 & 10 & 0 & 0 & 10 \\
        10 & 0 & 0 & -11 & 10 & 10 & 10 & 0 & 0 \\
        0 & 10 & 0 & 10 & -18 & 10 & 0 & 10 & 0 \\
        0 & 0 & 10 & 10 & 10 & -12 & 0 & 0 & 10 \\
        10 & 0 & 0 & 10 & 0 & 0 & -19 & 10 & 10 \\
        0 & 10 & 0 & 0 & 10 & 0 & 10 & -14 & 10 \\
        0 & 0 & 10 & 0 & 0 & 10 & 10 & 10 & -13
    \end{pmatrix}
\end{align*}
Solving this gives \(X = \begin{pmatrix}
    0 & 0 & 1 & 0 & 1 & 0 & 1 & 0 & 0
\end{pmatrix}^T\)
and so we have \begin{align*}
    P = \begin{pmatrix}
        0 & 0 & 1 \\
        0 & 1 & 0 \\
        1 & 0 & 0
    \end{pmatrix}, \quad PA = \begin{pmatrix}
        7 & 8 & 7 \\
        4 & 2 & 6 \\
        7 & 9 & 1
    \end{pmatrix}
\end{align*}
which tells us that the minimum cost is \(\text{tr}(PA) = 10\).
\subsection{Sudoku}

In the game of Sudoku we are given a partially filled \(9 \times 9\) grid (81 squares) and the aim is to populate the remainder of the grid such that every column, row and box contains each of the numbers from 1 to 9. These boxes split the \(9 \times 9\) grid into 9 non-overlapping \(3 \times 3\) grids.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{sudoku.png}
    \caption{Medium Sudoku puzzle from New York Times on June 24, 2024}
    \label{fig:Sudoku 1}
\end{figure}

\noindent We can formulate the Sudoku problem as follows. Following \cite[p.~1]{mücke2024sudoku} the vector \(\mathbf{x}\) will have 9 binary entries for each Sudoku square \(x_{i,j,k} = x_{81i + 9j + k}\). So each variable \(x_{i,j,k}\) is defined as
\begin{align*}
    x_{i,j,k} = \begin{cases}
        1 & \text{if row \(i\) column \(j\) has value \(k\)} \\
        0 & \text{otherwise.}
    \end{cases}
\end{align*}
The constraints of this problem are:
\begin{center}
\begin{tabular}{ |c|c| } 
 \hline
   & Constraint\\ 
 \hline
 1 & Each column contains 1 to 9\\ 
 2 & Each row contains 1 to 9\\
 3 & Each box contains 1 to 9\\
 4 & Every square has a value\\
 5 & Some squares have been filled\\
 \hline
\end{tabular}
\end{center}
THIS BOX COULD BE WRONG

\noindent Using \cite[p.~326]{ILPsudoku} the constraints become
\begin{equation*}
\begin{aligned} 
\text{(1)} \quad \sum_{i=1}^9 x_{i,j,k} &= 1\quad \forall j,k \in \{1 \dots 9\} \\
\end{aligned}
\end{equation*}


\begin{equation*}
\begin{aligned} 
&\text{(2)} \quad \sum_{j=1}^9 x_{i,j,k} &&= 1\quad \forall i,k \in \{1 \dots 9\} \\
&\text{(3)} \quad \sum_{j=3q-2}^{3q} \sum_{i=3p-2}^{3p} x_{i,j,k} &&= 1\quad \forall k \in \{1 \dots 9\}, \forall p,q \in \{1 \dots 3\}\\
&\text{(4)} \quad \sum_{k=1}^9 x_{i,j,k} &&= 1\quad \forall i,j \in \{1 \dots 9\} \\
&\text{(5)} \quad x_f &&= 1\quad \forall f \in F
\end{aligned}
\end{equation*}
where \(F\) is the set containing all triples corresponding to the indices of the filled squares. \\

\noindent Now we transform these constraints into penalties by subtracting 1 from both sides and squaring them. If we consider an binary vector \(\mathbf{v}\) with \(n\) entries, we find that  
\begin{align*}
    \bigg(\sum_{i=1}^n v_i - 1\bigg)^2 &= \sum_{i=1}^n v_i \sum_{i=1}^n v_i - 2\sum_{i=1}^n v_i + 1 \\
    &= \sum_{i=1}^n \sum_{j=1}^n v_iv_j - 2\sum_{i=1}^n v_i^2 + 1 \\
    &= \mathbf{v}^T\begin{pmatrix}
        1 & 1 & \dots \\
        1 & 1 & \dots \\
        \vdots & \vdots & \ddots
    \end{pmatrix} \mathbf{v} + \mathbf{v}^T(-2 I)\mathbf{v} + 1 \\
    &= \mathbf{v}^T (\text{ones} (n, n) - 2I_n )\mathbf{v} + 1
\end{align*}





\subsection{Example: Rubik's cube}

\newpage

\section{Graph Isomorphism and Matching problems}
\subsection{The Graph Isomorphism problem}
Two graphs \(G_A\) and \(G_B\) are isomorphic iff there exists a bijection between the vertices of the graphs such that every edge connecting vertices in \(G_A\) corresponds to a edge connecting vertices in \(G_B\). \\
Essentially, isomorphism means each graph can be rearranged and relabelled to look identical to the other. \\

\noindent Look at these two graphs, 


\noindent First we need to establish some results from \autocite{klus2023continuous}.
\begin{defn}\label{def:1}
    \cite[p.~6]{klus2023continuous} Suppose we have graphs \(G_A\) and \(G_B\) with adjacency matrices \(A\) and \(B\) respectively. The doubly stochastic relaxation of the graph isomorphism problem can be formulated as
    \begin{equation*}
        c_D = \min_{X \in D(n)} ||XA - BX||^2_F
    \end{equation*}
    where \(D(n)\) is the set of doubly stochastic matrices and \(||\cdot||_F\) denotes the Frobenius norm.
\end{defn}

\begin{lem}
    \cite[p.~13]{klus2023continuous} If we penalize non-binary matrices, this problem can be written as \begin{align*}
    \min_{\substack{\mathbf{x}\geq 0 \\ C\mathbf{x} = d \\ H\mathbf{x} = 0}} -\mathbf{x}^T \mathbf{x}
    \end{align*}
    with \(\mathbf{x} = \begin{bmatrix}
        X_1 \\
        \vdots \\
        X_n
    \end{bmatrix}\) where \(X_1, \dots X_n\) are the columns of \(X\) and \(C\), \(d\), and H are defined as shown on \cite[p.~8]{klus2023continuous}.
\end{lem}

\noindent To obtain a QUBO formulation of this problem we can transform the constraints into penalties as shown below.
\begin{center}
\begin{tabular}{ |c|c|c| } 
 \hline
   & Constraint  & Penalty \\ 
 \hline
 1 & \(C\mathbf{x} = d\) & \((C\mathbf{x} - d)\cdot(C\mathbf{x} - d)\) \\ 
 2 & \(H\mathbf{x} = 0\) & \(H \mathbf{x} \cdot H \mathbf{x} \) \\ 
 \hline
\end{tabular}
\end{center}
These penalties are 0 for functions which satisfy the constraints and positively-valued otherwise. Therefore by scaling each penalty by the weights \(\lambda_1\) and \(\lambda_2\) then adding them to the objective function we can ensure that the optimal solution to the QUBO problem corresponds to a isomorphism (if it exists).\\

\noindent Penalty 1 can be simplified to,
\begin{align*}
    (C\mathbf{x} - d)\cdot (C\mathbf{x} - d) &= C\mathbf{x} \cdot C\mathbf{x} - 2d\cdot C\mathbf{x} - d\cdot d \\
    &= \mathbf{x}^T C^T C \mathbf{x} - 2d\cdot Cx - d\cdot d.
\end{align*}
But \(\mathbf{x}\) is a binary vector, so \(\mathbf{x}_i = \mathbf{x}_i^2\) for all \(i\) and therefore the linear term \(-2d\cdot C\mathbf{x}\) can be expressed as a quadratic term. So we can write \(-2d\cdot C \mathbf{x} = -2\mathbf{x}^T\text{diag}(C^Td)\mathbf{x}\) and thus penalty 1 becomes
\begin{align*}
    (C\mathbf{x} - d)\cdot (C\mathbf{x} - d) = \mathbf{x}^T( C^T C -2\text{diag}(C^T d))\mathbf{x} - d\cdot d
\end{align*}
where diag\((v)\) is the function which maps the \(i\)th entry of \(v\) to the \(i\)th diagonal entry of a diagonal matrix.\\

\noindent Penalty 2 can be written as,
\begin{align*}
    H\mathbf{x} \cdot H\mathbf{x} = \mathbf{x}^T H^T H \mathbf{x}.
\end{align*}

\noindent So the QUBO formulation of this problem is 
\begin{align*}
    &-\mathbf{x}^T \mathbf{x} + \lambda_1(C\mathbf{x} - d)\cdot (C\mathbf{x} - d)  +\lambda_2 H\mathbf{x} \cdot H\mathbf{x}\\
    = &-\mathbf{x}^T \mathbf{x} + \lambda_1(\mathbf{x}^T( C^T C -2\text{diag}(C^T d))\mathbf{x} - d\cdot d) + \lambda_2 \mathbf{x}^T H^T H \mathbf{x}\\
    = &\hspace{5pt} \mathbf{x}^T (-I + \lambda_1( C^T C -2\text{diag}(C^T d)) + 
    \lambda_2 H^T H) \mathbf{x} - \lambda_1 d \cdot d.
\end{align*}\\
We can drop the constant \(- \lambda_1 d \cdot d\) at the end and then write the problem as
\begin{align*}
    \min \: f(\mathbf{x}) = \mathbf{x}^T Q \mathbf{x}
\end{align*}
where \(Q = -I + \lambda_1( C^T C -2\text{diag}(C^T d)) + \lambda_2 H^T H\).\\

\noindent Note that depending on the penalty weights used the optimal solution may not correspond to an isomorphism. So each optimal solution should be tested against the constraints.

\subsection{Example: Graph Isomorphism problem}






\nocite{*}
\printbibliography % This command prints the cited references.

\end{document}
